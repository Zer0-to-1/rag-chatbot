# ğŸ¼ RAG Chatbot â€” Baby Steps Edition (Windows/Mac/Linux)

This is a **beginnerâ€‘friendly** project that lets you build a working **AI chatbot** that can answer questions about **your own PDFs**.
You do **not** need deep ML knowledge. Just follow the steps exactly.

---

## ğŸ§  What is RAG?
**R**etrievalâ€‘**A**ugmented **G**eneration = The chatbot first **retrieves** relevant text from your documents, then the AI model **generates** an answer using that context.
Think of it as an **openâ€‘book exam**: the bot checks your PDFs before answering.

---

## âœ… What youâ€™ll get
- A local chatbot you can run on your laptop.
- Upload PDFs and ask questions. The bot answers **with sources**.
- Clean, minimal **Streamlit UI**.
- No paid APIs. Uses a **free local model** via **Ollama**.

---

## ğŸ› ï¸ What you need (install once)
1) **Python 3.10+** (Windows/Mac/Linux).  
2) **Git** (optional, nice to have).  
3) **Ollama** (runs small open models locally): install from the official Ollama website, then **open it** so itâ€™s running in the background.

> After installing Ollama, open **Terminal / PowerShell** and run **one** of these to download a model:
```
ollama pull mistral     # good default
# or
ollama pull llama3      # if available on your machine
```

---

## â–¶ï¸ How to run (copyâ€“paste)
Open **PowerShell (Windows)** or **Terminal (Mac/Linux)** and run:

```bash
# 1) Go to the project folder
cd rag_chatbot_baby_steps

# 2) (Optional but recommended) Create a virtual environment
python -m venv .venv
# Windows:
.\.venv\Scripts\activate
# Mac/Linux:
# source .venv/bin/activate

# 3) Install Python packages
pip install -r requirements.txt

# 4) Start the app
streamlit run app.py
```

Then open the browser tab that appears (usually http://localhost:8501).

---

## ğŸ§¾ How to use the app
1) Make sure **Ollama** is running (you installed it and itâ€™s open).
2) In the app, **upload one or more PDFs** (syllabus, notes, manuals, etc.).
3) Ask a question in the text box (e.g., â€œWhat are the deadlines?â€).
4) The bot shows an answer **and the exact source chunks** it used.

---

## ğŸ“ Project structure
```
rag_chatbot_baby_steps/
â”œâ”€ app.py                # Streamlit UI + RAG pipeline
â”œâ”€ requirements.txt      # Python dependencies
â”œâ”€ README.md             # This file (step-by-step guide)
â””â”€ vectordb/             # (auto-created) where your document embeddings live
```

---

## ğŸ§© How it works (simple)
- We split your PDFs into small **chunks** of text.
- We convert each chunk into a number vector (â€œ**embedding**â€) using **Sentence Transformers**.
- We store vectors in a small database (**Chroma**).
- When you ask a question: we embed your question â†’ **find the most similar chunks** â†’
  build a **prompt** with those chunks â†’ ask the local model in **Ollama** to answer.
- We show the answer **and** the sources.

---

## ğŸ§¯ Common fixes
- **Model not found** â†’ run `ollama pull mistral` in Terminal/PowerShell.
- **Ollama not running** â†’ open the Ollama app OR run `ollama serve`.
- **GPU errors** â†’ Ollama will fall back to CPU; itâ€™s slower but fine for learning.
- **Long PDFs** â†’ you can upload multiple, but start small to keep it snappy.

---

## ğŸ™ Credits
- Local LLM runtime powered by **Ollama**.
- Text embeddings by **sentence-transformers** (`all-MiniLM-L6-v2`).
- Vector DB is **Chroma**.
- UI built with **Streamlit**.

